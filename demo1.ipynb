{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output file names\n",
    "input_file = '1159300_Q_Day.Cmd.txt'\n",
    "output_file = '1159300_Q_Day_filtered.Cmd.txt'\n",
    "\n",
    "# Open the input file for reading with latin-1 encoding\n",
    "with open(input_file, 'r', encoding='latin-1') as infile:\n",
    "    # Open the output file for writing\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        # Read the file line by line\n",
    "        data_section = False\n",
    "        for line in infile:\n",
    "            # Check for the start of the data section\n",
    "            if line.strip() == \"# DATA\":\n",
    "                data_section = True\n",
    "                continue  # Skip the # DATA line\n",
    "\n",
    "            # Process lines only after the data section has started\n",
    "            if data_section:\n",
    "                # Split the line into components by the delimiter\n",
    "                components = line.strip().split(';')\n",
    "\n",
    "                # Ensure we have the expected number of components (3)\n",
    "                if len(components) == 3:\n",
    "                    date = components[0]\n",
    "                    value = components[2].strip()\n",
    "\n",
    "                    # Write the date and value to the output file\n",
    "                    outfile.write(f\"{date}    {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Filtered files saved in 'station_data_filtered' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the input and output directories\n",
    "input_folder = 'station_data'\n",
    "output_folder = 'station_data_filtered'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all .txt files in the input folder\n",
    "input_files = glob.glob(os.path.join(input_folder, '*.txt'))\n",
    "\n",
    "# Process each file\n",
    "for input_file in input_files:\n",
    "    # Get the base name of the file (e.g., '1159300_Q_Day.Cmd.txt')\n",
    "    base_name = os.path.basename(input_file)\n",
    "    \n",
    "    # Define the corresponding output file path\n",
    "    output_file = os.path.join(output_folder, base_name)\n",
    "    \n",
    "    # Open the input file for reading with latin-1 encoding\n",
    "    with open(input_file, 'r', encoding='latin-1') as infile:\n",
    "        # Open the output file for writing\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            # Read the file line by line\n",
    "            data_section = False\n",
    "            for line in infile:\n",
    "                # Check for the start of the data section\n",
    "                if line.strip() == \"# DATA\":\n",
    "                    data_section = True\n",
    "                    continue  # Skip the # DATA line\n",
    "\n",
    "                # Process lines only after the data section has started\n",
    "                if data_section:\n",
    "                    # Split the line into components by the delimiter\n",
    "                    components = line.strip().split(';')\n",
    "\n",
    "                    # Ensure we have the expected number of components (3)\n",
    "                    if len(components) == 3:\n",
    "                        date = components[0]\n",
    "                        value = components[2].strip()\n",
    "\n",
    "                        # Write the date and value to the output file\n",
    "                        outfile.write(f\"{date}    {value}\\n\")\n",
    "\n",
    "print(\"Processing complete. Filtered files saved in 'station_data_filtered' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/master_storage2/kishan/Climate-Information-System/.conda/lib/python3.11/site-packages/xarray/backends/plugins.py:159: RuntimeWarning: 'netcdf4' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "/master_storage2/kishan/Climate-Information-System/.conda/lib/python3.11/site-packages/xarray/backends/plugins.py:159: RuntimeWarning: 'scipy' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "/master_storage2/kishan/Climate-Information-System/.conda/lib/python3.11/site-packages/xarray/backends/plugins.py:168: RuntimeWarning: 'netcdf4' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "/master_storage2/kishan/Climate-Information-System/.conda/lib/python3.11/site-packages/xarray/backends/plugins.py:168: RuntimeWarning: 'h5netcdf' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "/master_storage2/kishan/Climate-Information-System/.conda/lib/python3.11/site-packages/xarray/backends/plugins.py:168: RuntimeWarning: 'scipy' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'scipy']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Loop over each NetCDF file\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m nc_files:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Load the daily NetCDF file\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Resample the data to monthly and annual sums\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     monthly_ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mresample(time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/master_storage2/kishan/Climate-Information-System/.conda/lib/python3.11/site-packages/xarray/backends/api.py:552\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(backend_kwargs)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    555\u001b[0m     from_array_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/master_storage2/kishan/Climate-Information-System/.conda/lib/python3.11/site-packages/xarray/backends/plugins.py:197\u001b[0m, in \u001b[0;36mguess_engine\u001b[0;34m(store_spec)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound the following matches with the input file in xarray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms IO \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackends: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompatible_engines\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. But their dependencies may not be installed, see:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/user-guide/io.html \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m     )\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'scipy']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# List of NetCDF file paths\n",
    "nc_files = [\n",
    "    '/home/kishan/datahub/climate/GCM_DS1/EM_imd/historical/pr_1979_2014.nc'\n",
    "    '/home/kishan/datahub/meteorology/IMD/pr_interpolated_125.nc'\n",
    "    '/home/kishan/datahub/climate/GCM_DS1/EM/ssp245_pr/pr_2015_2100.nc'\n",
    "    # '/home/kishan/datahub/climate/GCM_DS1/EM/ssp545_pr/pr_2015_2100.nc'\n",
    "\n",
    "    '/home/kishan/datahub/climate/GCM_DS1/EM/historical/tasmax_1979_2014.nc'\n",
    "    '/home/kishan/datahub/meteorology/IMDAA/tasmax.nc'\n",
    "    '/home/kishan/datahub/climate/GCM_DS1/EM/ssp245/tasmax_2015_2100.nc'\n",
    "    # '/home/kishan/datahub/climate/GCM_DS1/EM/ssp585/tasmax_2015_2100.nc'\n",
    "\n",
    "    '/home/kishan/datahub/climate/GCM_DS1/EM/historical/tasmin_1979_2014.nc'\n",
    "    '/home/kishan/datahub/meteorology/IMDAA/tasmin.nc'\n",
    "    '/home/kishan/datahub/climate/GCM_DS1/EM/ssp245/tasmin_2015_2100.nc'\n",
    "    # '/home/kishan/datahub/climate/GCM_DS1/EM/ssp585/tasmin_2015_2100.nc'\n",
    "]\n",
    "\n",
    "# Define the output directory where the resampled files will be saved\n",
    "output_base_dir = '/home/kishan/datahub/climate/GCM_DS1/temp'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "# Loop over each NetCDF file\n",
    "for file_path in nc_files:\n",
    "    # Load the daily NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Resample the data to monthly and annual sums\n",
    "    monthly_ds = ds.resample(time='M').mean()\n",
    "    annual_ds = ds.resample(time='Y').mean()\n",
    "\n",
    "    # Generate the new file names with \"_monthly\" and \"_yearly\" tags\n",
    "    file_dir, file_name = os.path.split(file_path)\n",
    "    base_name, ext = os.path.splitext(file_name)\n",
    "\n",
    "    # Define the full paths for the new files\n",
    "    monthly_file_path = os.path.join(output_base_dir, f\"{base_name}_monthly{ext}\")\n",
    "    annual_file_path = os.path.join(output_base_dir, f\"{base_name}_yearly{ext}\")\n",
    "\n",
    "    # Save the resampled datasets to new NetCDF files in the output directory\n",
    "    monthly_ds.to_netcdf(monthly_file_path)\n",
    "    annual_ds.to_netcdf(annual_file_path)\n",
    "\n",
    "    print(f\"Processed and saved: {monthly_file_path}\")\n",
    "    print(f\"Processed and saved: {annual_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
